config_file=./BERT/data/funnel_transformer/net_config_base.json
init_checkpoint=bert_pretrain/open_domain/pretrained_model/funnel_transformer/B4-4-4H768-ELEC-FULL-TF/model.ckpt
vocab_file=./BERT/data/chinese_L-12_H-768_A-12/vocab.txt
label_id=./BERT/data/lcqmc/label_dict.json
max_length=128
train_file=GLUE/proc_data/rte/pretrain_proc_data_glue_rte_vocab.uncased.txt.len-128.train.tfrecord
dev_file=GLUE/proc_data/rte/pretrain_proc_data_glue_rte_vocab.uncased.txt.len-128.dev.tfrecord
model_output=GLUE/proc_data/rte/model_4_4_4_v6_new_2
epoch=10
num_classes=2
train_size=2480
eval_size=1000
batch_size=16
model_type=funnelbert
model_scope=model
if_shard=2
is_debug=1
run_type=estimator
opt_type="all_reduce"
num_gpus=1
parse_type=parse_batch
rule_model=normal
profiler="no"
train_op=adam_weight_decay_exclude
running_type=train
cross_tower_ops_type=paisoar
distribution_strategy=MirroredStrategy
load_pretrained=yes
warmup=warmup
decay=decay
with_target=""
input_target=""
distillation="normal"
temperature=2.0
distillation_ratio=1.0
num_hidden_layers=12
task_type=single_sentence_classification
classifier=order_classifier
mode="single_task"
multi_task_type="wsdm,ccks,ant,xnli,lcqmc,chnsenticorp"
multi_task_config="./BERT/t2t_bert/distributed_multitask/multi_task.json"
task_invariant=no
init_lr=2e-5
ln_type=postln