config_file=./BERT/data/roberta_zh_l12/bert_config_tiny_structural_attention.json
init_checkpoint="bert_pretrain/open_domain/pretrain_single_random_hard_gan/gpt/chinese_tiny/model.ckpt-429680"
vocab_file=./BERT/data/roberta_zh_l12/vocab.txt
label_id=./BERT/data/lcqmc/label_dict.json
max_length=512
train_file=bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_0.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_1.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_2.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_3.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_4.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_5.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_6.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_7.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_8.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_9.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_10.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_11.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_12.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_13.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_14.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_15.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_16.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_17.tfrecords
dev_file=bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_18.tfrecords,bert_pretrain/open_domain/pretrain_single_random_hard_gan/chunk_19.tfrecords
model_output=bert_pretrain/open_domain/pretrain_single_random_hard_gan/debug/bert_glance_mlm_confusion_set_v2_structural_attention/
epoch=5
num_classes=2
train_size=11000000
eval_size=1280000
batch_size=32
model_type=bert
model_scope=bert
if_shard=1
is_debug=1
run_type=estimator
opt_type="all_reduce"
num_gpus=1
parse_type=parse_dynamic
rule_model=normal
profiler="no"
train_op=adam_weight_decay_exclude
running_type=train
cross_tower_ops_type=paisoar
distribution_strategy=MirroredStrategy
load_pretrained=no
warmup=warmup
decay=decay
with_target=""
input_target=""
distillation="normal"
temperature=2.0
distillation_ratio=1.0
num_hidden_layers=12
task_type=bert_pretrain
classifier=order_classifier
max_predictions_per_seq=78
ln_type=postln
mode="single_task"
multi_task_type="generator,discriminator"
multi_task_config="./BERT/t2t_bert/pretrain_finetuning/multi_model_config_gumbel_sharing_embedding_tiny.json"
joint_train=1
init_lr=1e-4
electra_mode=gumbel_training
train_op_type=alternate
optimization_type=grl
sharing_mode=none
gumbel_anneal=none
gen_disc_type=equal_not_equal_disc_loss_all
minmax_mode=corrupted
attention_type=normal_attention
confusion_set_path=./BERT/data/chinese_L-12_H-768_A-12/vocab_confusion_id.txt
confusion_set_mask_path=./BERT/data/chinese_L-12_H-768_A-12/vocab_confusion_id_mask.txt